# Default values for qwen3-8b-triton
# This is a YAML-formatted file.

global:
  namespace: triton-inference
  appName: qwen3-8b

# Namespace configuration
namespace:
  create: true
  name: triton-inference
  labels:
    name: triton-inference
    app: qwen3-8b

# Resource quotas and limits
resourceQuota:
  enabled: true
  requests:
    cpu: "2"
    memory: "2Gi"
  limits:
    cpu: "2"
    memory: "3Gi"
  persistentvolumeclaims: "3"

limitRange:
  enabled: true
  default:
    cpu: "500m"
    memory: "512Mi"
  defaultRequest:
    cpu: "250m"
    memory: "256Mi"
  max:
    cpu: "2"
    memory: "4Gi"
  min:
    cpu: "100m"
    memory: "128Mi"

# Triton Inference Server configuration
triton:
  enabled: true
  name: triton-qwen3-8b
  
  image:
    repository: nvcr.io/nvidia/tritonserver
    tag: 24.12-trtllm-python-py3
    pullPolicy: IfNotPresent
  
  replicas: 1
  
  config:
    modelRepo: "/models"
    httpPort: "8000"
    grpcPort: "8001"
    metricsPort: "8002"
    logVerbose: "1"
    logInfo: "true"
    logWarning: "true"
    logError: "true"
  
  nodeSelector:
    cloud.google.com/gke-accelerator: nvidia-tesla-a100
  
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
    - key: cloud.google.com/gke-spot
      operator: Equal
      value: "true"
      effect: NoSchedule
  
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: "32Gi"
      cpu: "8"
    limits:
      nvidia.com/gpu: 1
      memory: "64Gi"
      cpu: "16"
  
  livenessProbe:
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3
  
  readinessProbe:
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  
  service:
    type: LoadBalancer
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 8002
    sessionAffinity: ClientIP
    sessionAffinityTimeout: 10800
    internal:
      enabled: true
      type: ClusterIP
  
  storage:
    enabled: true
    size: 500Gi
    storageClass: premium-rwo
    accessMode: ReadWriteOnce

# OpenWebUI configuration
openwebui:
  enabled: true
  name: openwebui
  
  image:
    repository: ghcr.io/open-webui/open-webui
    tag: main
    pullPolicy: IfNotPresent
  
  replicas: 1
  
  config:
    webuiName: "Qwen 3 8B Inference"
    webuiUrl: "http://localhost:8080"
    openaiApiBaseUrl: "http://triton-qwen3-8b-internal:8000"
    openaiApiKey: "not-required"
    defaultModel: "qwen3-8b"
    defaultModels: "qwen3-8b"
    enableSignup: "true"
    enableCommunitySharing: "false"
    logLevel: "INFO"
    tritonService: "triton-qwen3-8b-internal"
    tritonHttpPort: "8000"
    tritonGrpcPort: "8001"
    tritonModelName: "qwen3_8b"
  
  nodeSelector:
    accelerator: cpu
    pool: cpu
  
  tolerations:
    - key: workload-type
      operator: Equal
      value: cpu
      effect: NoSchedule
  
  resources:
    requests:
      memory: "512Mi"
      cpu: "500m"
    limits:
      memory: "1Gi"
      cpu: "1000m"
  
  livenessProbe:
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  
  readinessProbe:
    initialDelaySeconds: 20
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
  
  service:
    type: LoadBalancer
    port: 80
    targetPort: 8080
    sessionAffinity: ClientIP
    sessionAffinityTimeout: 10800
    internal:
      enabled: true
      type: ClusterIP
      port: 8080
  
  storage:
    enabled: true
    size: 10Gi
    storageClass: premium-rwo
    accessMode: ReadWriteOnce

# Prometheus configuration
prometheus:
  enabled: true
  name: prometheus
  
  image:
    repository: prom/prometheus
    tag: v2.48.0
    pullPolicy: IfNotPresent
  
  replicas: 1
  
  nodeSelector:
    accelerator: cpu
    pool: cpu
  
  tolerations:
    - key: workload-type
      operator: Equal
      value: cpu
      effect: NoSchedule
  
  resources:
    requests:
      memory: "1Gi"
      cpu: "1000m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  
  livenessProbe:
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  
  readinessProbe:
    initialDelaySeconds: 15
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
  
  service:
    type: LoadBalancer
    port: 9090
    sessionAffinity: ClientIP
    sessionAffinityTimeout: 10800
    internal:
      enabled: true
      type: ClusterIP
      port: 9090
  
  storage:
    enabled: true
    size: 20Gi
    storageClass: premium-rwo
    accessMode: ReadWriteOnce
  
  config:
    scrapeInterval: 15s
    scrapeTimeout: 10s
    evaluationInterval: 15s
    retentionTime: 30d
    retentionSize: 15GB
    clusterName: trt-llm-cluster
    environment: production

# Upload job for model files
uploadJob:
  enabled: false  # Set to true when you need to upload models
  sleepDuration: 3600  # Sleep duration in seconds (1 hour default)

